[
  {
    "objectID": "posts/mvg-linear-regression/index.html",
    "href": "posts/mvg-linear-regression/index.html",
    "title": "Data Fun",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport re\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay, f1_score, classification_report\nfrom joblib import Parallel, delayed\n\n\nDATA_PATH = \"../../data/\"\nMVG_FILE = \"subway_only_dedup.parquet\"\nEVENTS_FILE = \"events.csv\"\nHOLIDAYS_FILE = \"public_holiday.csv\"\nALLIANZ_FILE = \"allianz_arena_events.csv\"\nWEATHER_FILE = \"weather_2024-2025.csv\"\n\n\ndef load_data(path, mvg_file, events_file, holidays_file, allianz_file, weather_file):\n    mvg_data = pd.read_parquet(path + mvg_file)\n    events_df = pd.read_csv(path + events_file, parse_dates=['start_date', 'end_date'])\n    public_holiday_df = pd.read_csv(path + holidays_file, parse_dates=['date'])\n    allianz_arena_df = pd.read_csv(path + allianz_file, parse_dates=['date'])\n    weather_df = pd.read_csv(path + weather_file, parse_dates=['date'])\n    return mvg_data, events_df, public_holiday_df, allianz_arena_df, weather_df\n\nmvg_data_orig, events_df, public_holiday_df, allianz_arena_df, weather_df = load_data(\n    DATA_PATH, MVG_FILE, EVENTS_FILE, HOLIDAYS_FILE, ALLIANZ_FILE, WEATHER_FILE\n)\n\n\nmvg_data = mvg_data_orig\n\nmvg_data = mvg_data.drop(columns=['transportType', 'realtimeDepartureTime', 'timestamp'])\nmvg_data = mvg_data[mvg_data.realtime]\nmvg_data['onTime'] = mvg_data['delayInMinutes'] == 0\n\nmvg_datetime = pd.to_datetime(mvg_data['plannedDepartureTime']).dt\nmvg_data['plannedDepartureTime_dt'] = pd.to_datetime(mvg_data['plannedDepartureTime'])\n\n#mvg_data['hourOfDay'] = mvg_datetime.hour\n\nmvg_data['minuteOfDay'] = mvg_datetime.hour * 60 + mvg_datetime.minute\n# encode minute of day as 23:00 and 00:00 are far apart but close in reality\nmvg_data['minuteSin'] = np.sin(2 * np.pi * mvg_data['minuteOfDay'] / 1440)\nmvg_data['minuteCos'] = np.cos(2 * np.pi * mvg_data['minuteOfDay'] / 1440)\n\n\nmvg_data['dayOfWeek'] = mvg_datetime.day_of_week # Monday=0, Sunday=6\n#mvg_data['dayOfYear'] = mvg_datetime.day_of_year\n\nmvg_data['isRushHour'] = mvg_datetime.hour.between(7, 9) | mvg_datetime.hour.between(16, 18)\nmvg_data['isWeekend'] = mvg_data['dayOfWeek'] &gt;= 5 # Saturday=5, Sunday=6\n\n# remove negative delay for bucketing\nmvg_data = mvg_data[mvg_data['delayInMinutes'] &gt;= 0]\n\n# only use u6\nmvg_data = mvg_data[mvg_data['label'] == 'U6']\n\n# include events\nmvg_data['isMajorEvent'] = False\nfor index, event in events_df.iterrows():\n    event_mask = (\n        (mvg_data['plannedDepartureTime_dt'].dt.date &gt;= event['start_date'].date()) &\n        (mvg_data['plannedDepartureTime_dt'].dt.date &lt;= event['end_date'].date())\n    )\n    mvg_data.loc[event_mask, 'isMajorEvent'] = True\nprint(\"\\nEvent day departures vs. normal day departures:\")\nprint(mvg_data['isMajorEvent'].value_counts())\n\n\n# set if public holiday is true\nholiday_dates = set(public_holiday_df['date'].dt.date)\nmvg_data['isHoliday'] = mvg_data['plannedDepartureTime_dt'].dt.date.isin(holiday_dates)\nprint(\"\\nHoliday departures vs. normal day departures:\")\nprint(mvg_data['isHoliday'].value_counts())\n\n# include allianz arena events\nallianz_arena_dates = set(allianz_arena_df['date'].dt.date)\nmvg_data['isAllianzArenaEvent'] = mvg_data['plannedDepartureTime_dt'].dt.date.isin(allianz_arena_dates)\nprint(\"\\nAllianz Arena Event departures vs. normal day departures:\")\nprint(mvg_data['isAllianzArenaEvent'].value_counts())\n\n#include weather\nweather_df['date'] = pd.to_datetime(weather_df['date'], format='%d.%m.%Y')\nmvg_data = pd.merge(\n    mvg_data,\n    weather_df,\n    how='left',\n    left_on=mvg_data['plannedDepartureTime_dt'].dt.date,\n    right_on=weather_df['date'].dt.date\n)\nmvg_data = mvg_data.drop(columns=['key_0', 'date'])\nnan_weather_rows = mvg_data['tempAvg'].isnull().sum()\nif nan_weather_rows &gt; 0:\n    print(f\"Warning: {nan_weather_rows} rows did not have matching weather data and contain NaNs.\")\n\n\n#bins = [float('-inf'), 2, np.inf]\n#labels = ['On Time', 'Delayed']\n#mvg_data['delayCategory'] = pd.cut(mvg_data['delayInMinutes'], bins=bins, labels=labels)\n\nprint(\"New binary class distribution:\\n\", mvg_data['onTime'].value_counts())\n\n\nEvent day departures vs. normal day departures:\nisMajorEvent\nFalse    3204709\nTrue      249221\nName: count, dtype: int64\n\nHoliday departures vs. normal day departures:\nisHoliday\nFalse    3386613\nTrue       67317\nName: count, dtype: int64\n\nAllianz Arena Event departures vs. normal day departures:\nisAllianzArenaEvent\nFalse    3153279\nTrue      300651\nName: count, dtype: int64\nNew binary class distribution:\n onTime\nTrue     3239816\nFalse     214114\nName: count, dtype: int64\n\n\n\nmvg_data.sample(5)\n\n\n\n\n\n\n\n\nstation\nplannedDepartureTime\nrealtime\ndelayInMinutes\nlabel\ndestination\nonTime\nplannedDepartureTime_dt\nminuteOfDay\nminuteSin\n...\nisMajorEvent\nisHoliday\nisAllianzArenaEvent\ntempMin\ntempMax\ntempAvg\nrainfall\nsunHours\nwindMax\nsnowfall\n\n\n\n\n412014\nde:09162:1430\n2025-08-02 15:30:00+00:00\nTrue\n0\nU6\nMünchner Freiheit\nTrue\n2025-08-02 15:30:00+00:00\n930\n-0.793353\n...\nFalse\nFalse\nTrue\n12.9\n17.3\n14.6\n23.6\n0.1\n28.0\n0\n\n\n3337806\nde:09162:540\n2025-02-27 18:55:00+00:00\nTrue\n6\nU6\nGoetheplatz\nFalse\n2025-02-27 18:55:00+00:00\n1135\n-0.971342\n...\nFalse\nFalse\nFalse\n1.6\n7.2\n3.8\n0.4\n3.3\n33.0\n0\n\n\n277398\nde:09162:1340\n2024-11-19 06:46:00+00:00\nTrue\n0\nU6\nKlinikum Großhadern\nTrue\n2024-11-19 06:46:00+00:00\n406\n0.979925\n...\nFalse\nFalse\nFalse\n6.5\n12.5\n9.8\n12.9\n1.0\n63.0\n0\n\n\n3078564\nde:09184:480\n2024-03-21 18:34:00+00:00\nTrue\n0\nU6\nGarching, Forschungszentrum\nTrue\n2024-03-21 18:34:00+00:00\n1114\n-0.989016\n...\nFalse\nFalse\nFalse\n7.3\n14.7\n10.1\n5.5\n2.1\n49.0\n0\n\n\n2579365\nde:09162:80\n2025-03-16 07:08:00+00:00\nTrue\n0\nU6\nGarching, Forschungszentrum\nTrue\n2025-03-16 07:08:00+00:00\n428\n0.956305\n...\nFalse\nFalse\nFalse\n1.6\n4.4\n3.2\n0.3\n0.0\n32.0\n0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\nplt.figure(figsize=(8, 8))\nscatter = plt.scatter(\n    mvg_data['minuteSin'],\n    mvg_data['minuteCos'],\n    c=mvg_data['minuteOfDay'], # Color points by the original minute of the day\n    cmap='viridis'            # Use a nice color map\n)\n\nplt.gca().set_aspect('equal', adjustable='box')\n\nplt.title('Cyclical Encoding of Minute of Day')\nplt.xlabel('Sine(Minute of Day)')\nplt.ylabel('Cosine(Minute of Day)')\n\n# Add a colorbar to show what the colors mean\ncbar = plt.colorbar(scatter)\ncbar.set_label('Minute of Day')\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nmvg_data.sample(5)\n\n\n\n\n\n\n\n\nstation\nplannedDepartureTime\nrealtime\ndelayInMinutes\nlabel\ndestination\nonTime\nplannedDepartureTime_dt\nminuteOfDay\nminuteSin\n...\nisMajorEvent\nisHoliday\nisAllianzArenaEvent\ntempMin\ntempMax\ntempAvg\nrainfall\nsunHours\nwindMax\nsnowfall\n\n\n\n\n1981834\nde:09162:50\n2024-05-28 17:37:00+00:00\nTrue\n0\nU6\nKlinikum Großhadern\nTrue\n2024-05-28 17:37:00+00:00\n1057\n-0.994969\n...\nFalse\nFalse\nFalse\n10.5\n16.7\n13.8\n0.0\n3.4\n33.0\n0\n\n\n3255570\nde:09162:540\n2024-10-07 17:01:00+00:00\nTrue\n0\nU6\nKlinikum Großhadern\nTrue\n2024-10-07 17:01:00+00:00\n1021\n-0.967046\n...\nFalse\nFalse\nFalse\n9.7\n23.3\n15.1\n0.0\n8.0\n22.0\n0\n\n\n470431\nde:09162:1470\n2025-03-29 23:12:00+00:00\nTrue\n0\nU6\nFürstenried West\nTrue\n2025-03-29 23:12:00+00:00\n1392\n-0.207912\n...\nFalse\nFalse\nTrue\n6.1\n9.2\n7.7\n4.0\n0.0\n32.0\n0\n\n\n973839\nde:09162:1130\n2024-02-27 08:36:00+00:00\nTrue\n0\nU6\nGarching, Forschungszentrum\nTrue\n2024-02-27 08:36:00+00:00\n516\n0.777146\n...\nFalse\nFalse\nFalse\n3.0\n8.0\n5.3\n0.0\n1.1\n20.0\n0\n\n\n1494489\nde:09162:420\n2024-10-16 14:19:00+00:00\nTrue\n0\nU6\nKlinikum Großhadern\nTrue\n2024-10-16 14:19:00+00:00\n859\n-0.569997\n...\nFalse\nFalse\nFalse\n9.2\n17.2\n12.3\n0.0\n2.7\n29.0\n0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\nfeatures = mvg_data.drop(['delayInMinutes', 'onTime', 'plannedDepartureTime', 'plannedDepartureTime_dt', 'realtime', 'minuteOfDay', 'onTime', 'tempMax', 'tempMin'], axis=1)\ntarget = mvg_data['onTime']\n\nfeatures_encoded = pd.get_dummies(features, columns=['station', 'label', 'destination'])\nfeatures_encoded.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in features_encoded.columns]\nprint(f\"Columns after sanitization: {features_encoded.shape[1]}\")\nfeatures_encoded = features_encoded.loc[:, ~features_encoded.columns.duplicated()]\nprint(f\"Columns after removing duplicates: {features_encoded.shape[1]}\")\n\n\nprint(f\"Total rows in the full dataset: {len(features_encoded)}\")\n\nX_sample, _, y_sample, _ = train_test_split(features_encoded, target, train_size=5_000, random_state=0, stratify=target)\nX_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X_sample, y_sample, test_size=0.3, random_state=0)\nX_val_sample, X_test_sample, y_val_sample, y_test_sample = train_test_split(X_test_sample, y_test_sample, test_size=0.5, random_state=0)\n\nprint(f\"\\nSampled training set size: {len(X_train_sample)}\")\nprint(f\"Sampled validation set size: {len(X_val_sample)}\")\nprint(f\"Sampled test set size: {len(X_test_sample)}\")\n\n\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(features_encoded, target, test_size=0.3, random_state=0)\nX_val_full, X_test_full, y_val_full, y_test_full = train_test_split(X_test_full, y_test_full, test_size=0.5, random_state=0)\n\nprint(f\"\\nFull training set size: {len(X_train_full)}\")\nprint(f\"Full validation set size: {len(X_val_full)}\")\nprint(f\"Full test set size: {len(X_test_full)}\")\n\ntrain_data_full = pd.concat([X_train_full, y_train_full], axis=1)\ntarget_column_name = y_train_full.name\n\nontime_samples = train_data_full[train_data_full[target_column_name]]\ndelay_samples = train_data_full[~train_data_full[target_column_name]]\n\nontime_downsampled = ontime_samples.sample(n=len(delay_samples), random_state=0)\n\ntrain_data_balanced = pd.concat([ontime_downsampled, delay_samples])\n\nX_train_balanced = train_data_balanced.drop(columns=[target_column_name])\ny_train_balanced = train_data_balanced[target_column_name]\n\nprint(\"\\nOriginal full training class distribution:\")\nprint(y_train_full.value_counts())\nprint(\"\\nBalanced training class distribution:\")\nprint(y_train_balanced.value_counts())\n\nColumns after sanitization: 73\nColumns after removing duplicates: 72\nTotal rows in the full dataset: 3453930\n\nSampled training set size: 3500\nSampled validation set size: 750\nSampled test set size: 750\n\nFull training set size: 2417751\nFull validation set size: 518089\nFull test set size: 518090\n\nOriginal full training class distribution:\nonTime\nTrue     2268021\nFalse     149730\nName: count, dtype: int64\n\nBalanced training class distribution:\nonTime\nTrue     149730\nFalse    149730\nName: count, dtype: int64\n\n\n\ndef train_and_evaluate_rf(params, X_train, y_train, X_val, y_val):\n    \"\"\"Trains a Random Forest and returns both train and validation F1-scores.\"\"\"\n    model = RandomForestClassifier(\n        **params,\n        n_jobs=-1,\n        random_state=0\n    )\n    #model = lgb.LGBMClassifier(\n    #    random_state=0,\n    #    n_jobs=-1,\n    #    **params\n    #)\n    model.fit(X_train, y_train)\n\n    val_pred = model.predict(X_val)\n    train_pred = model.predict(X_train)\n\n    # Calculate F1-score specifically for the 'Delayed' class (which is 'False')\n    # This is the metric we want to maximize to improve precision\n    train_f1 = f1_score(y_train, train_pred, pos_label=False)\n\n    # Calculate F1-score specifically for the 'Delayed' class (which is 'False')\n    # This is the metric we want to maximize to improve precision\n    val_f1 = f1_score(y_val, val_pred, pos_label=False)\n\n    return params, train_f1, val_f1\n\n\ntrain_data_sample = pd.concat([X_train_sample, y_train_sample], axis=1)\ntarget_column_name_sample = y_train_sample.name\n\nontime_samples_s = train_data_sample[train_data_sample[target_column_name_sample]]\ndelay_samples_s = train_data_sample[~train_data_sample[target_column_name_sample]]\nontime_downsampled_s = ontime_samples_s.sample(n=len(delay_samples_s), random_state=0)\ntrain_data_balanced_s = pd.concat([ontime_downsampled_s, delay_samples_s])\n\nX_train_sample_balanced = train_data_balanced_s.drop(columns=[target_column_name_sample])\ny_train_sample_balanced = train_data_balanced_s[target_column_name_sample]\n\n\n#estimators_to_test = range(10, 501, 10)\nparam_grid_definition = {\n    'n_estimators': range(10, 501, 10),\n    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 20],\n    'max_depth': [10, 15, 20, None],\n}\nparams_to_test = list(ParameterGrid(param_grid_definition))\nprint(f\"Generated {len(params_to_test)} parameter combinations to test.\")\nprint(\"First 3 combinations:\")\nprint(params_to_test[:3])\n\n\nresults = Parallel(n_jobs=-1)(\n    delayed(train_and_evaluate_rf)(\n        params,\n        X_train_sample_balanced,\n        y_train_sample_balanced,\n        X_val_sample,\n        y_val_sample\n    ) for params in params_to_test\n)\nresults_df = pd.DataFrame(results, columns=['params', 'train_f1_score', 'validation_f1_score'])\n\nGenerated 1400 parameter combinations to test.\nFirst 3 combinations:\n[{'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 10}, {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 20}, {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 30}]\n\n\n\nbest_params, _, best_val_f1_score = max(results, key=lambda item: item[2])\n\nprint(\"\\n--- Hyperparameter Tuning Complete ---\")\nprint(f\"Best Validation F1-Score (for 'Delayed' class): {best_val_f1_score:.4f}\")\nprint(\"Best Parameters Found:\")\nprint(best_params)\n\ndef format_params(p_dict):\n    return (f\"n_est:{p_dict.get('n_estimators', 'N/A')}, \"\n            f\"min_samples_leaf:{p_dict.get('min_samples_leaf', 'N/A')}, \"\n            f\"depth:{p_dict.get('max_depth', 'N/A')}\")\nresults_df['param_label'] = results_df['params'].apply(format_params)\n\ndf_melted = results_df.melt(\n    id_vars='param_label',\n    value_vars=['train_f1_score', 'validation_f1_score'],\n    var_name='score_type',\n    value_name='f1_score'\n)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='f1_score', y='param_label', hue='score_type', data=df_melted, orient='h')\n\nplt.title('LightGBM Tuning: Training vs. Validation F1-Scores')\nplt.xlabel(\"F1-Score for 'Delayed' Class\")\nplt.ylabel(\"Parameter Combination\")\nplt.legend(title='Score Type')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()\n\n\n--- Hyperparameter Tuning Complete ---\nBest Validation F1-Score (for 'Delayed' class): 0.2018\nBest Parameters Found:\n{'max_depth': 20, 'min_samples_leaf': 2, 'n_estimators': 160}\n\n\n\n\n\n\n\n\n\n\n#best_parameter = max(validation_results, key=validation_results.get)\n#\n#best_accuracy = validation_results[best_parameter]\n#\n#print(f\"The best parameter is: {best_parameter}\")\n#print(f\"With a validation accuracy of: {best_accuracy:.4f}\")\n\n\nfinal_model = RandomForestClassifier(\n    **best_params,\n    n_jobs=-1,\n    verbose=1,\n    random_state=0\n)\n#final_model = lgb.LGBMClassifier(\n#    **best_params,\n#    n_jobs=-1,\n#    verbose=-1,\n#    random_state=0\n#)\n\nprint(f\"Training final model on {len(X_train_balanced)} samples...\")\nfinal_model.fit(X_train_balanced, y_train_balanced)\n\nTraining final model on 299460 samples...\n\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    5.1s finished\n\n\nRandomForestClassifier(max_depth=20, min_samples_leaf=2, n_estimators=160,\n                       n_jobs=-1, random_state=0, verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n160\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n20\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n2\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \n-1\n\n\n\nrandom_state \n0\n\n\n\nverbose \n1\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\nimportances = final_model.feature_importances_\nfeature_names = X_train_full.columns\n\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n\nfeature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False).head(15)\n\nplt.figure(figsize=(10, 8))\nsns.barplot(x='importance', y='feature', data=feature_importance_df)\nplt.title('Top 15 Most Important Features')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ny_pred = final_model.predict(X_test_full)\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test_full, y_pred))\n\ndisp = ConfusionMatrixDisplay.from_estimator(\n    final_model,\n    X_test_full,\n    y_test_full,\n    cmap=plt.cm.Blues\n)\ndisp.ax_.set_title('Confusion Matrix')\nplt.show()\n\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 160 out of 160 | elapsed:    0.7s finished\n\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       False       0.19      0.85      0.31     32306\n        True       0.99      0.75      0.86    485784\n\n    accuracy                           0.76    518090\n   macro avg       0.59      0.80      0.58    518090\nweighted avg       0.94      0.76      0.82    518090\n\n\n\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 160 out of 160 | elapsed:    0.7s finished"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Fun",
    "section": "",
    "text": "encode minute of day as 23:00 and 00:00 are far apart but close in reality\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]